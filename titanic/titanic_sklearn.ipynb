{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Dataset with Scikit-Learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utilizing functions created earlier for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for preprocessing\n",
    "def extract_first_letter(cabin):\n",
    "\t'''\n",
    "\tExtracting the first letter from a given cabin number. If the input is\n",
    "\tdifferent, function returns 'NaN'.\n",
    "\t'''\n",
    "\tif type(cabin) == type('a'):\n",
    "\t\treturn cabin[0]\n",
    "\treturn 'X'\n",
    "\n",
    "def extract_cabin_number(data):\n",
    "\t'''\n",
    "\tExtracting the number of the cabin. If there's none, returns NaN.\n",
    "\t'''\n",
    "\tif data['Cabin_letter'] != 'X':\n",
    "\t\treturn data['Cabin'][1:]\n",
    "\treturn 0\n",
    "\n",
    "def title_extraction(title):\n",
    "\t'''\n",
    "\tExtracting the title from the 'Name' column.\n",
    "\t'''\n",
    "\ttemp = title.split()\n",
    "\n",
    "\tfor i in range(len(temp)):\n",
    "\t\tif '.' in temp[i]:\n",
    "\t\t\treturn temp[i].split('.')[0]\n",
    "\n",
    "def title_condensation(data):\n",
    "\t'''\n",
    "\tCondensing the amount of titles to smaller value.\n",
    "\t'''\n",
    "\tif data['Title'] in ['Miss', 'Mlle', 'Ms']:\n",
    "\t\treturn 'Miss'\n",
    "\t\n",
    "\telif data['Title'] in ['Mrs', 'Countess', 'Lady', 'Mme']:\n",
    "\t\treturn 'Mrs'\n",
    "\n",
    "\telif data['Title'] == 'Dr' and data['Sex'] == 'female':\n",
    "\t\treturn 'Mrs'\n",
    "\t\t\n",
    "\telse:\n",
    "\t\treturn 'Mr'\n",
    "\n",
    "def age_filler2(data):\n",
    "\t'''\n",
    "\tFills up the NaN with proper age mean according to \n",
    "\ttitle_condensed value. If age is already there, it's being\n",
    "\tcarried unchanged.\n",
    "\t'''\n",
    "\tmiss_mean = np.float64(data.groupby('Title_condensed')['Age'].mean()['Miss'])\n",
    "\tmr_mean = np.float64(data.groupby('Title_condensed')['Age'].mean()['Mr'])\n",
    "\tmrs_mean = np.float64(data.groupby('Title_condensed')['Age'].mean()['Mrs'])\n",
    "\n",
    "\tif data['Age_nan_True']:\n",
    "\t\tif data['Title_condensed'] == 'Miss':\n",
    "\t\t\treturn miss_mean\n",
    "\n",
    "\t\telif data['Title_condensed'] == 'Mr':\n",
    "\t\t\treturn mr_mean\n",
    "\n",
    "\t\telif data['Title_condensed'] == 'Mrs':\n",
    "\t\t\treturn mrs_mean\n",
    "\t\n",
    "\treturn data['Age']\n",
    "\n",
    "def age_filler(data):\n",
    "\tif data['Age_nan_True']:\n",
    "\t\tif data['Title_condensed'] == 'Mrs':\n",
    "\t\t\treturn 35.99\n",
    "\n",
    "\t\tif data['Title_condensed'] == 'Mr':\n",
    "\t\t\treturn 30.73\n",
    "\n",
    "\t\tif data['Title_condensed'] == 'Miss':\n",
    "\t\t\treturn 21.76\n",
    "\t\n",
    "\treturn np.float64(data['Age'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the data engineering part\n",
    "def preprocessing(titanic):\n",
    "\t'''\n",
    "\tFunction that preprocesses the data enginnering/feature engineering\n",
    "\tpart of loaded DataFrame.\n",
    "\t'''\t\n",
    "\n",
    "\tprint(\"PREPROCESSING...\")\n",
    "\n",
    "\t# Dropping top 0,5% of 'Fare' entries\n",
    "\tif len(titanic)>500:\n",
    "\t\tfor i in range(int(len(titanic)/200)):\n",
    "\t\t\ttitanic.drop(titanic['Fare'].idxmax(), axis=0, inplace=True)\n",
    "\n",
    "\tprint(\"Top 0,5% 'Fare' entries dropped\")\n",
    "\n",
    "\t# Extract cabin letter and number\n",
    "\ttitanic['Cabin_letter'] = titanic['Cabin'].apply(lambda x: extract_first_letter(x))\n",
    "\ttitanic['Cabin_number'] = titanic.apply(extract_cabin_number, axis=1)\n",
    "\t\n",
    "\t# Change type of cabin number column to numeric values\n",
    "\ttitanic['Cabin_number'] = pd.to_numeric(titanic['Cabin_number'], errors='ignore')\n",
    "\ttitanic['Cabin_number'].fillna(0)\n",
    "\tprint('Cabin column done')\n",
    "\n",
    "\t# Create family size value column\n",
    "\ttitanic['Family_size'] = titanic['SibSp'] + titanic['Parch'] + 1\n",
    "\tprint('Family size created')\n",
    "\n",
    "\t# Create Fare per person column\n",
    "\ttitanic['Fare_per_person'] = titanic['Fare'] / titanic['Family_size']\n",
    "\tprint('Fare per person created')\n",
    "\n",
    "\t# Extracting the title from the name and making the total number smaller\n",
    "\ttitanic['Title'] = titanic['Name'].apply(lambda x: title_extraction(x))\n",
    "\ttitanic['Title_condensed'] = titanic.apply(title_condensation, axis=1)\n",
    "\tprint('Titles extracted and condensed')\n",
    "\n",
    "\t# Calculating the mean age values per each title\n",
    "\t#miss_mean = titanic.groupby('Title_condensed')['Age'].mean()['Miss']\n",
    "\t#mr_mean = titanic.groupby('Title_condensed')['Age'].mean()['Mr']\n",
    "\t#mrs_mean = titanic.groupby('Title_condensed')['Age'].mean()['Mrs']\n",
    "\n",
    "\t# Filling the missing age values\n",
    "\ttitanic['Age_nan_True'] = titanic['Age'].isna()\n",
    "\ttitanic['Age_filled'] = titanic.apply(age_filler, axis=1)\n",
    "\tprint('Age column filled')\n",
    "\n",
    "\t# Filling the missing fare values\n",
    "\ttitanic['Fare'] = titanic['Fare'].fillna(titanic['Fare'].mean())\n",
    "\tprint('Filling the missing fare values')\n",
    "\n",
    "\t# Dropping unnecessary columns\n",
    "\tdel_cols = ['Cabin', 'Age', 'Age_nan_True', 'Title', 'Name', 'Ticket', 'PassengerId', 'Cabin_number']\n",
    "\ttitanic.drop(del_cols, axis=1, inplace=True)\t\n",
    "\tprint('Columns deleted')\n",
    "\n",
    "\tprint('PREPROCESSING DONE\\n')\n",
    "\n",
    "\t# Return the preprocessed DataFrame\n",
    "\treturn titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode the features and normalize the data in the set\n",
    "def onehot_normalize(data):\n",
    "\n",
    "\tfrom sklearn.compose import make_column_transformer\n",
    "\tfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\tfrom sklearn.model_selection import train_test_split\n",
    "\n",
    "\tprint('ONE-HOT ENCODING AND NORMALIZATION...')\n",
    "\n",
    "\t# Split the columns into linear and categorical ones\n",
    "\tlinear_cols = ['Pclass', 'SibSp', 'Parch', 'Fare', 'Family_size',\n",
    "\t\t\t\t\t'Fare_per_person', 'Age_filled']\n",
    "\t\n",
    "\tcategorical_cols = ['Sex', 'Embarked', 'Cabin_letter', 'Title_condensed']\n",
    "\n",
    "\t# Create a column transformer\n",
    "\tct = make_column_transformer(\n",
    "\t\t(MinMaxScaler(), linear_cols),\n",
    "\t\t(OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "\t)\n",
    "\tprint('Column transformer instentiated')\n",
    "\n",
    "\t# Create X and y sets\n",
    "\tX = data.drop('Survived', axis=1)\n",
    "\ty = data['Survived']\n",
    "\n",
    "\t# Split the data\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\tprint('Data split into train and test sets')\n",
    "\n",
    "\t# Fit the column transformer\n",
    "\tct.fit(X_train)\n",
    "\n",
    "\t# Transform the train and test sets\n",
    "\tX_train_norm = ct.transform(X_train)\n",
    "\tX_test_norm = ct.transform(X_test)\n",
    "\tprint('Columns transformed')\n",
    "\n",
    "\tprint('ONE-HOT ENCODING AND NORMALIZATION FINISHED')\n",
    "\n",
    "\treturn X_train_norm, X_test_norm, y_train, y_test, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode the features and normalize the data in the set\n",
    "def onehot_normalize_2(data):\n",
    "\n",
    "\tfrom sklearn.compose import make_column_transformer\n",
    "\tfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\tfrom sklearn.model_selection import train_test_split\n",
    "\n",
    "\tprint('ONE-HOT ENCODING AND NORMALIZATION...')\n",
    "\n",
    "\t# Split the columns into linear and categorical ones\n",
    "\tlinear_cols = ['Pclass', 'SibSp', 'Parch', 'Fare', 'Family_size',\n",
    "\t\t\t\t\t'Fare_per_person', 'Age_filled']\n",
    "\t\n",
    "\tcategorical_cols = ['Sex', 'Embarked', 'Cabin_letter', 'Title_condensed']\n",
    "\t\n",
    "\t# Create a column transformer\n",
    "\tct = make_column_transformer(\n",
    "\t\t(MinMaxScaler(), linear_cols),\n",
    "\t\t(OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "\t)\n",
    "\tprint('Column transformer instentiated')\n",
    "\n",
    "\t# Create X and y sets\n",
    "\tX = data.drop('Survived', axis=1)\n",
    "\t#X = pd.get_dummies(data=X)\n",
    "\ty = data['Survived']\n",
    "\n",
    "\t# Split the data\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\tprint('Data split into train and test sets')\n",
    "\n",
    "\t# Fit the column transformer\n",
    "\tct.fit(X_train)\n",
    "\n",
    "\t# Transform the train and test sets\n",
    "\tX_train_norm = ct.transform(X_train)\n",
    "\tX_test_norm = ct.transform(X_test)\n",
    "\tprint('Columns transformed')\n",
    "\n",
    "\tprint('ONE-HOT ENCODING AND NORMALIZATION FINISHED')\n",
    "\n",
    "\treturn X_train_norm, X_test_norm, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSING...\n",
      "Top 0,5% 'Fare' entries dropped\n",
      "Cabin column done\n",
      "Family size created\n",
      "Fare per person created\n",
      "Titles extracted and condensed\n",
      "Age column filled\n",
      "Filling the missing fare values\n",
      "Columns deleted\n",
      "PREPROCESSING DONE\n",
      "\n",
      "ONE-HOT ENCODING AND NORMALIZATION...\n",
      "Column transformer instentiated\n",
      "Data split into train and test sets\n",
      "Columns transformed\n",
      "ONE-HOT ENCODING AND NORMALIZATION FINISHED\n"
     ]
    }
   ],
   "source": [
    "# Load in the data\n",
    "titanic_train = pd.read_csv('train.csv')\n",
    "\n",
    "# Preprocessing\n",
    "titanic_train = preprocessing(titanic_train)\n",
    "\n",
    "# One-hot encoding, normalization, train/test split\n",
    "X_train, X_test, y_train, y_test = onehot_normalize_2(titanic_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic RFC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "# Creating a RFC model\n",
    "rfc_basic = RandomForestClassifier(random_state=42, verbose=0)\n",
    "\n",
    "# Fitting model to the data\n",
    "rfc_basic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, model_name, y_true):\n",
    "\n",
    "\t# Calculate predictions\n",
    "\tpred = model.predict(X_test)\n",
    "\n",
    "\t# Basic metrics\n",
    "\tprint(f'EVALUATION METRICS FOR MODEL: {model_name}\\n')\n",
    "\n",
    "\tprint('Confusion matrix: ')\n",
    "\tprint(confusion_matrix(y_test, pred))\n",
    "\n",
    "\tprint('\\nClassification report: ')\n",
    "\tprint(classification_report(y_test, pred))\n",
    "\n",
    "\tacc = accuracy_score(y_test, pred)\n",
    "\tf1 = accuracy_score(y_test, pred)\n",
    "\n",
    "\tprint('Scores for basic model')\n",
    "\tprint(f'Accuracy score: {acc:.3f}')\n",
    "\tprint(f'F1-score: {f1:.3f}')\n",
    "\t\"\"\"\n",
    "\t# Feature importance\n",
    "\tfeature_imp = pd.DataFrame(model.feature_importances_, \n",
    "\t\t\t\t\t\t\t\tindex = Xcolumns, \n",
    "\t\t\t\t\t\t\t\tcolumns=['Feature importance score']).sort_values(ascending=False,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tby=['Feature importance score'])\n",
    "\n",
    "\tsns.barplot(x=feature_imp['Feature importance score'], y=feature_imp.index)\n",
    "\t\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION METRICS FOR MODEL: rfc_basic\n",
      "\n",
      "Confusion matrix: \n",
      "[[87 19]\n",
      " [20 52]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.82       106\n",
      "           1       0.73      0.72      0.73        72\n",
      "\n",
      "    accuracy                           0.78       178\n",
      "   macro avg       0.77      0.77      0.77       178\n",
      "weighted avg       0.78      0.78      0.78       178\n",
      "\n",
      "Scores for basic model\n",
      "Accuracy score: 0.781\n",
      "F1-score: 0.781\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model=rfc_basic, model_name='rfc_basic', y_true=y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline to beat is 78.1%. Let's try using GridSearchCV to improve our score by trying different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'criterion': 'gini', 'max_depth': 7, 'max_features': 'sqrt', 'n_estimators': 1024}\n",
      "Best score: 0.835001498351813\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "\t'n_estimators': [128, 256, 512, 1024],\n",
    "\t'max_features': ['sqrt', 'log2'],\n",
    "\t'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "\t'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "}\n",
    "\n",
    "rfc_cv = RandomForestClassifier(random_state=42, verbose=0)\n",
    "\n",
    "cv_rfc = GridSearchCV(estimator=rfc_cv, \n",
    "\t\t\t\t\t  param_grid=param_grid,\n",
    "\t\t\t\t\t  cv=5)\n",
    "\n",
    "cv_rfc.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best params: {cv_rfc.best_params_}')\n",
    "print(f'Best score: {cv_rfc.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION METRICS FOR MODEL: rfc_cv\n",
      "\n",
      "Confusion matrix: \n",
      "[[91 15]\n",
      " [24 48]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.82       106\n",
      "           1       0.76      0.67      0.71        72\n",
      "\n",
      "    accuracy                           0.78       178\n",
      "   macro avg       0.78      0.76      0.77       178\n",
      "weighted avg       0.78      0.78      0.78       178\n",
      "\n",
      "Scores for basic model\n",
      "Accuracy score: 0.781\n",
      "F1-score: 0.781\n"
     ]
    }
   ],
   "source": [
    "# Training the RFC with GridSearchCV best params found\n",
    "rfc_cv = RandomForestClassifier(random_state=42, \n",
    "\t\t\t\t\t\t\t\tverbose=0,\n",
    "\t\t\t\t\t\t\t\tcriterion='gini',\n",
    "\t\t\t\t\t\t\t\tmax_depth=10,\n",
    "\t\t\t\t\t\t\t\tmax_features='sqrt',\n",
    "\t\t\t\t\t\t\t\tn_estimators=1024)\n",
    "\n",
    "rfc_cv.fit(X_train, y_train)\n",
    "\n",
    "evaluate_model(model=rfc_cv, model_name='rfc_cv', y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

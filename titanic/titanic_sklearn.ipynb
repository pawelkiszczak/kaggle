{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Dataset with Scikit-Learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utilizing functions created earlier for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for preprocessing\n",
    "def extract_first_letter(cabin):\n",
    "\t'''\n",
    "\tExtracting the first letter from a given cabin number. If the input is\n",
    "\tdifferent, function returns 'NaN'.\n",
    "\t'''\n",
    "\tif type(cabin) == type('a'):\n",
    "\t\treturn cabin[0]\n",
    "\treturn 'X'\n",
    "\n",
    "def extract_cabin_number(data):\n",
    "\t'''\n",
    "\tExtracting the number of the cabin. If there's none, returns NaN.\n",
    "\t'''\n",
    "\tif data['Cabin_letter'] != 'X':\n",
    "\t\treturn data['Cabin'][1:]\n",
    "\treturn 0\n",
    "\n",
    "def title_extraction(title):\n",
    "\t'''\n",
    "\tExtracting the title from the 'Name' column.\n",
    "\t'''\n",
    "\ttemp = title.split()\n",
    "\n",
    "\tfor i in range(len(temp)):\n",
    "\t\tif '.' in temp[i]:\n",
    "\t\t\treturn temp[i].split('.')[0]\n",
    "\n",
    "def title_condensation(data):\n",
    "\t'''\n",
    "\tCondensing the amount of titles to smaller value.\n",
    "\t'''\n",
    "\tif data['Title'] in ['Miss', 'Mlle', 'Ms']:\n",
    "\t\treturn 'Miss'\n",
    "\t\n",
    "\telif data['Title'] in ['Mrs', 'Countess', 'Lady', 'Mme']:\n",
    "\t\treturn 'Mrs'\n",
    "\n",
    "\telif data['Title'] == 'Dr' and data['Sex'] == 'female':\n",
    "\t\treturn 'Mrs'\n",
    "\t\t\n",
    "\telse:\n",
    "\t\treturn 'Mr'\n",
    "\n",
    "def age_filler2(data):\n",
    "\t'''\n",
    "\tFills up the NaN with proper age mean according to \n",
    "\ttitle_condensed value. If age is already there, it's being\n",
    "\tcarried unchanged.\n",
    "\t'''\n",
    "\tmiss_mean = np.float64(data.groupby('Title_condensed')['Age'].mean()['Miss'])\n",
    "\tmr_mean = np.float64(data.groupby('Title_condensed')['Age'].mean()['Mr'])\n",
    "\tmrs_mean = np.float64(data.groupby('Title_condensed')['Age'].mean()['Mrs'])\n",
    "\n",
    "\tif data['Age_nan_True']:\n",
    "\t\tif data['Title_condensed'] == 'Miss':\n",
    "\t\t\treturn miss_mean\n",
    "\n",
    "\t\telif data['Title_condensed'] == 'Mr':\n",
    "\t\t\treturn mr_mean\n",
    "\n",
    "\t\telif data['Title_condensed'] == 'Mrs':\n",
    "\t\t\treturn mrs_mean\n",
    "\t\n",
    "\treturn data['Age']\n",
    "\n",
    "def age_filler(data):\n",
    "\tif data['Age_nan_True']:\n",
    "\t\tif data['Title_condensed'] == 'Mrs':\n",
    "\t\t\treturn 35.99\n",
    "\n",
    "\t\tif data['Title_condensed'] == 'Mr':\n",
    "\t\t\treturn 30.73\n",
    "\n",
    "\t\tif data['Title_condensed'] == 'Miss':\n",
    "\t\t\treturn 21.76\n",
    "\t\n",
    "\treturn np.float64(data['Age'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the data engineering part\n",
    "def preprocessing(titanic):\n",
    "\t'''\n",
    "\tFunction that preprocesses the data enginnering/feature engineering\n",
    "\tpart of loaded DataFrame.\n",
    "\t'''\t\n",
    "\n",
    "\tprint(\"PREPROCESSING...\")\n",
    "\n",
    "\t# Dropping top 0,5% of 'Fare' entries\n",
    "\tif len(titanic)>500:\n",
    "\t\tfor i in range(int(len(titanic)/200)):\n",
    "\t\t\ttitanic.drop(titanic['Fare'].idxmax(), axis=0, inplace=True)\n",
    "\n",
    "\tprint(\"Top 0,5% 'Fare' entries dropped\")\n",
    "\n",
    "\t# Extract cabin letter and number\n",
    "\ttitanic['Cabin_letter'] = titanic['Cabin'].apply(lambda x: extract_first_letter(x))\n",
    "\ttitanic['Cabin_number'] = titanic.apply(extract_cabin_number, axis=1)\n",
    "\t\n",
    "\t# Change type of cabin number column to numeric values\n",
    "\ttitanic['Cabin_number'] = pd.to_numeric(titanic['Cabin_number'], errors='ignore')\n",
    "\ttitanic['Cabin_number'].fillna(0)\n",
    "\tprint('Cabin column done')\n",
    "\n",
    "\t# Create family size value column\n",
    "\ttitanic['Family_size'] = titanic['SibSp'] + titanic['Parch'] + 1\n",
    "\tprint('Family size created')\n",
    "\n",
    "\t# Create Fare per person column\n",
    "\ttitanic['Fare_per_person'] = titanic['Fare'] / titanic['Family_size']\n",
    "\tprint('Fare per person created')\n",
    "\n",
    "\t# Extracting the title from the name and making the total number smaller\n",
    "\ttitanic['Title'] = titanic['Name'].apply(lambda x: title_extraction(x))\n",
    "\ttitanic['Title_condensed'] = titanic.apply(title_condensation, axis=1)\n",
    "\tprint('Titles extracted and condensed')\n",
    "\n",
    "\t# Calculating the mean age values per each title\n",
    "\t#miss_mean = titanic.groupby('Title_condensed')['Age'].mean()['Miss']\n",
    "\t#mr_mean = titanic.groupby('Title_condensed')['Age'].mean()['Mr']\n",
    "\t#mrs_mean = titanic.groupby('Title_condensed')['Age'].mean()['Mrs']\n",
    "\n",
    "\t# Filling the missing age values\n",
    "\ttitanic['Age_nan_True'] = titanic['Age'].isna()\n",
    "\ttitanic['Age_filled'] = titanic.apply(age_filler, axis=1)\n",
    "\tprint('Age column filled')\n",
    "\n",
    "\t# Filling the missing fare values\n",
    "\ttitanic['Fare'] = titanic['Fare'].fillna(titanic['Fare'].mean())\n",
    "\tprint('Filling the missing fare values')\n",
    "\n",
    "\t# Dropping unnecessary columns\n",
    "\tdel_cols = ['Cabin', 'Age', 'Age_nan_True', 'Title', 'Name', 'Ticket', 'PassengerId', 'Cabin_number']\n",
    "\ttitanic.drop(del_cols, axis=1, inplace=True)\t\n",
    "\tprint('Columns deleted')\n",
    "\n",
    "\tprint('PREPROCESSING DONE\\n')\n",
    "\n",
    "\t# Return the preprocessed DataFrame\n",
    "\treturn titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode the features and normalize the data in the set\n",
    "def onehot_normalize(data):\n",
    "\n",
    "\tfrom sklearn.compose import make_column_transformer\n",
    "\tfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\tfrom sklearn.model_selection import train_test_split\n",
    "\n",
    "\tprint('ONE-HOT ENCODING AND NORMALIZATION...')\n",
    "\n",
    "\t# Split the columns into linear and categorical ones\n",
    "\tlinear_cols = ['Pclass', 'SibSp', 'Parch', 'Fare', 'Family_size',\n",
    "\t\t\t\t\t'Fare_per_person', 'Age_filled']\n",
    "\t\n",
    "\tcategorical_cols = ['Sex', 'Embarked', 'Cabin_letter', 'Title_condensed']\n",
    "\n",
    "\t# Create a column transformer\n",
    "\tct = make_column_transformer(\n",
    "\t\t(MinMaxScaler(), linear_cols),\n",
    "\t\t(OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "\t)\n",
    "\tprint('Column transformer instentiated')\n",
    "\n",
    "\t# Create X and y sets\n",
    "\tX = data.drop('Survived', axis=1)\n",
    "\ty = data['Survived']\n",
    "\n",
    "\t# Split the data\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\tprint('Data split into train and test sets')\n",
    "\n",
    "\t# Fit the column transformer\n",
    "\tct.fit(X_train)\n",
    "\n",
    "\t# Transform the train and test sets\n",
    "\tX_train_norm = ct.transform(X_train)\n",
    "\tX_test_norm = ct.transform(X_test)\n",
    "\tprint('Columns transformed')\n",
    "\n",
    "\tprint('ONE-HOT ENCODING AND NORMALIZATION FINISHED')\n",
    "\n",
    "\treturn X_train_norm, X_test_norm, y_train, y_test, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode the features and normalize the data in the set\n",
    "def onehot_normalize_2(data):\n",
    "\n",
    "\tfrom sklearn.compose import make_column_transformer\n",
    "\tfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\tfrom sklearn.model_selection import train_test_split\n",
    "\n",
    "\tprint('ONE-HOT ENCODING AND NORMALIZATION...')\n",
    "\n",
    "\t# Split the columns into linear and categorical ones\n",
    "\tlinear_cols = ['Pclass', 'SibSp', 'Parch', 'Fare', 'Family_size',\n",
    "\t\t\t\t\t'Fare_per_person', 'Age_filled']\n",
    "\t\n",
    "\tcategorical_cols = ['Sex', 'Embarked', 'Cabin_letter', 'Title_condensed']\n",
    "\t\n",
    "\t# Create a column transformer\n",
    "\tct = make_column_transformer(\n",
    "\t\t(MinMaxScaler(), linear_cols),\n",
    "\t\t(OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "\t)\n",
    "\tprint('Column transformer instentiated')\n",
    "\n",
    "\t# Create X and y sets\n",
    "\tX = data.drop('Survived', axis=1)\n",
    "\t#X = pd.get_dummies(data=X)\n",
    "\ty = data['Survived']\n",
    "\n",
    "\t# Split the data\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\tprint('Data split into train and test sets')\n",
    "\n",
    "\t# Fit the column transformer\n",
    "\tct.fit(X_train)\n",
    "\n",
    "\t# Transform the train and test sets\n",
    "\tX_train_norm = ct.transform(X_train)\n",
    "\tX_test_norm = ct.transform(X_test)\n",
    "\tprint('Columns transformed')\n",
    "\n",
    "\tprint('ONE-HOT ENCODING AND NORMALIZATION FINISHED')\n",
    "\n",
    "\treturn X_train_norm, X_test_norm, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSING...\n",
      "Top 0,5% 'Fare' entries dropped\n",
      "Cabin column done\n",
      "Family size created\n",
      "Fare per person created\n",
      "Titles extracted and condensed\n",
      "Age column filled\n",
      "Filling the missing fare values\n",
      "Columns deleted\n",
      "PREPROCESSING DONE\n",
      "\n",
      "ONE-HOT ENCODING AND NORMALIZATION...\n",
      "Column transformer instentiated\n",
      "Data split into train and test sets\n",
      "Columns transformed\n",
      "ONE-HOT ENCODING AND NORMALIZATION FINISHED\n"
     ]
    }
   ],
   "source": [
    "# Load in the data\n",
    "titanic_train = pd.read_csv('train.csv')\n",
    "\n",
    "# Preprocessing\n",
    "titanic_train = preprocessing(titanic_train)\n",
    "\n",
    "# One-hot encoding, normalization, train/test split\n",
    "X_train, X_test, y_train, y_test = onehot_normalize_2(titanic_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic RFC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "# Creating a RFC model\n",
    "rfc_basic = RandomForestClassifier(random_state=42, verbose=0)\n",
    "\n",
    "# Fitting model to the data\n",
    "rfc_basic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, model_name, y_true):\n",
    "\n",
    "\t# Calculate predictions\n",
    "\tpred = model.predict(X_test)\n",
    "\n",
    "\t# Basic metrics\n",
    "\tprint(f'EVALUATION METRICS FOR MODEL: {model_name}\\n')\n",
    "\n",
    "\tprint('Confusion matrix: ')\n",
    "\tprint(confusion_matrix(y_true, pred))\n",
    "\n",
    "\tprint('\\nClassification report: ')\n",
    "\tprint(classification_report(y_true, pred))\n",
    "\n",
    "\tacc = accuracy_score(y_true, pred)\n",
    "\tf1 = accuracy_score(y_true, pred)\n",
    "\n",
    "\tprint('Scores for basic model')\n",
    "\tprint(f'Accuracy score: {acc:.3f}')\n",
    "\tprint(f'F1-score: {f1:.3f}')\n",
    "\t\"\"\"\n",
    "\t# Feature importance\n",
    "\tfeature_imp = pd.DataFrame(model.feature_importances_, \n",
    "\t\t\t\t\t\t\t\tindex = Xcolumns, \n",
    "\t\t\t\t\t\t\t\tcolumns=['Feature importance score']).sort_values(ascending=False,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tby=['Feature importance score'])\n",
    "\n",
    "\tsns.barplot(x=feature_imp['Feature importance score'], y=feature_imp.index)\n",
    "\t\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION METRICS FOR MODEL: rfc_basic\n",
      "\n",
      "Confusion matrix: \n",
      "[[87 19]\n",
      " [20 52]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.82       106\n",
      "           1       0.73      0.72      0.73        72\n",
      "\n",
      "    accuracy                           0.78       178\n",
      "   macro avg       0.77      0.77      0.77       178\n",
      "weighted avg       0.78      0.78      0.78       178\n",
      "\n",
      "Scores for basic model\n",
      "Accuracy score: 0.781\n",
      "F1-score: 0.781\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model=rfc_basic, model_name='rfc_basic', y_true=y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline to beat is 78.1%. Let's try using GridSearchCV to improve our score by trying different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13852.39s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "13852.40s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "13852.41s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "13852.42s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "13852.43s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "13852.44s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "13852.44s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "13852.45s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'criterion': 'gini', 'max_depth': 7, 'max_features': 'sqrt', 'n_estimators': 1024}\n",
      "Best score: 0.835001498351813\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "\t'n_estimators': [256, 512, 1024],\n",
    "\t'max_features': ['sqrt', 'log2'],\n",
    "\t'max_depth': [5, 6, 7, 8, 9, 10],\n",
    "\t'criterion': ['gini', 'log_loss']\n",
    "}\n",
    "\n",
    "rfc_cv = RandomForestClassifier(random_state=42, verbose=0, n_jobs=-1)\n",
    "\n",
    "cv_rfc = GridSearchCV(estimator=rfc_cv, \n",
    "\t\t\t\t\t  param_grid=param_grid)\n",
    "\n",
    "cv_rfc.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best params: {cv_rfc.best_params_}')\n",
    "print(f'Best score: {cv_rfc.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.835001498351813\n",
      "RandomForestClassifier(max_depth=7, n_estimators=1024, n_jobs=-1,\n",
      "                       random_state=42)\n",
      "{'criterion': 'gini', 'max_depth': 7, 'max_features': 'sqrt', 'n_estimators': 1024}\n",
      "<function BaseSearchCV._select_best_index at 0x2803c89d0>\n"
     ]
    }
   ],
   "source": [
    "print(cv_rfc.best_score_)\n",
    "print(cv_rfc.best_estimator_)\n",
    "print(cv_rfc.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION METRICS FOR MODEL: rfc_cv\n",
      "\n",
      "Confusion matrix: \n",
      "[[93 13]\n",
      " [28 44]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82       106\n",
      "           1       0.77      0.61      0.68        72\n",
      "\n",
      "    accuracy                           0.77       178\n",
      "   macro avg       0.77      0.74      0.75       178\n",
      "weighted avg       0.77      0.77      0.76       178\n",
      "\n",
      "Scores for basic model\n",
      "Accuracy score: 0.770\n",
      "F1-score: 0.770\n"
     ]
    }
   ],
   "source": [
    "# Training the RFC with GridSearchCV best params found\n",
    "rfc_cv = RandomForestClassifier(random_state=42, \n",
    "\t\t\t\t\t\t\t\tverbose=0,\n",
    "\t\t\t\t\t\t\t\tcriterion=cv_rfc.best_params_['criterion'],\n",
    "\t\t\t\t\t\t\t\tmax_depth=cv_rfc.best_params_['max_depth'],\n",
    "\t\t\t\t\t\t\t\tmax_features=cv_rfc.best_params_['max_features'],\n",
    "\t\t\t\t\t\t\t\tn_estimators=cv_rfc.best_params_['n_estimators'])\n",
    "\n",
    "rfc_cv.fit(X_train, y_train)\n",
    "\n",
    "evaluate_model(model=rfc_cv, model_name='rfc_cv', y_true=y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other classifiers to try out:\n",
    "* CatBoost\n",
    "* Gradient Boosting Trees\n",
    "* Decision Tree\n",
    "* Logistic Regression\n",
    "* Naive Bayes\n",
    "* KNN\n",
    "* Linear SVC\n",
    "* Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION METRICS FOR MODEL: gnb\n",
      "\n",
      "Confusion matrix: \n",
      "[[93 13]\n",
      " [40 32]]\n",
      "\n",
      "Classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.88      0.78       106\n",
      "           1       0.71      0.44      0.55        72\n",
      "\n",
      "    accuracy                           0.70       178\n",
      "   macro avg       0.71      0.66      0.66       178\n",
      "weighted avg       0.70      0.70      0.68       178\n",
      "\n",
      "Scores for basic model\n",
      "Accuracy score: 0.702\n",
      "F1-score: 0.702\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "gnb.predict(X_test)\n",
    "\n",
    "evaluate_model(model=gnb, model_name='gnb', y_true=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def append_data(df, data):\n",
    "    \"\"\"\n",
    "    Appends data to the bottom of a Pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to which data will be appended.\n",
    "        data (list or pd.DataFrame): The data to append. If a list is passed, it should be a list of lists \n",
    "                                     or a list of dictionaries, where each element represents a row of data.\n",
    "                                     If a DataFrame is passed, it should have the same columns as the DataFrame\n",
    "                                     to which data will be appended.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the appended data.\n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        # If data is already a DataFrame, we can simply concatenate it to the original DataFrame\n",
    "        new_df = pd.concat([df, data], ignore_index=True)\n",
    "    elif isinstance(data, list):\n",
    "        # If data is a list, we need to convert it to a DataFrame first\n",
    "        new_df = pd.concat([df, pd.DataFrame(data, columns=df.columns)], ignore_index=True)\n",
    "    else:\n",
    "        raise ValueError(\"Data must be either a list or a DataFrame.\")\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [226], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m results \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mModel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m rfc \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame({\u001b[39m'\u001b[39;49m\u001b[39mModel\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mrfc\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mAccuracy\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m0.8\u001b[39;49m})\n\u001b[1;32m      3\u001b[0m results, rfc\n\u001b[1;32m      4\u001b[0m \u001b[39m#append_data(results, rfc)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:662\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    656\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    657\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    658\u001b[0m     )\n\u001b[1;32m    660\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    661\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 662\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    663\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    664\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/internals/construction.py:493\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    491\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 493\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/internals/construction.py:118\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    116\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    119\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/internals/construction.py:656\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    655\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m indexes \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m raw_lengths:\n\u001b[0;32m--> 656\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIf using all scalar values, you must pass an index\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    658\u001b[0m \u001b[39melif\u001b[39;00m have_series:\n\u001b[1;32m    659\u001b[0m     index \u001b[39m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['Model', 'Accuracy'])\n",
    "rfc = pd.DataFrame({'Model': 'rfc', 'Accuracy': 0.8})\n",
    "results, rfc\n",
    "#append_data(results, rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
